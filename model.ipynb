{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Karishni\\Desktop\\Spring-Semester\\DDP\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#this file trains the model on the original dataset (all genres)\n",
    "import torch\n",
    "import datasets\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from transformers import BigBirdTokenizer, \\\n",
    "BigBirdForSequenceClassification, Trainer, TrainingArguments,EvalPrediction, AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>script</th>\n",
       "      <th>imdb user rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A NIGHT AT THE ROXBURY written by Steve Ko...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AT FIRST SIGHTEXT. VALLEY - DUSK Gold light da...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BamboozledbySpike LeeBLACK SCREENWe HEAR the v...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>THE BIG LEBOWSKIWe are floating up a steep scr...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Boys on the SideSCENE 1JANEThank you.  I'm Jan...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              script  imdb user rating\n",
       "0      A NIGHT AT THE ROXBURY written by Steve Ko...                 6\n",
       "1  AT FIRST SIGHTEXT. VALLEY - DUSK Gold light da...                 6\n",
       "2  BamboozledbySpike LeeBLACK SCREENWe HEAR the v...                 6\n",
       "3  THE BIG LEBOWSKIWe are floating up a steep scr...                 8\n",
       "4  Boys on the SideSCENE 1JANEThank you.  I'm Jan...                 6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your dataset\n",
    "df = pd.read_csv('scripts.csv')\n",
    "\n",
    "df=df[['script', 'imdb user rating']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.Dataset.from_pandas(df)\n",
    "\n",
    "# Split the dataset into train (80%) and temporary (20%)\n",
    "train_testvalid = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# Split the temporary dataset into validation and test datasets (50% each)\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "# Prepare the final DatasetDict\n",
    "final_datasets = datasets.DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'validation': test_valid['train'],\n",
    "    'test': test_valid['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1907/1907 [03:09<00:00, 10.08 examples/s]\n",
      "Map: 100%|██████████| 238/238 [00:22<00:00, 10.79 examples/s]\n",
      "Map: 100%|██████████| 239/239 [00:23<00:00, 10.14 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BigBirdTokenizer.from_pretrained('google/bigbird-roberta-base')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['script'], padding=\"max_length\", truncation=True, max_length=4096)\n",
    "\n",
    "# Apply the tokenize function to all splits\n",
    "final_datasets = final_datasets.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 1907/1907 [00:00<00:00, 3028.47 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 238/238 [00:00<00:00, 3058.66 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 239/239 [00:00<00:00, 3245.77 examples/s]\n"
     ]
    }
   ],
   "source": [
    "final_datasets.save_to_disk('data_train_test_val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Load datasets from disk\n",
    "final_datasets = load_from_disk('data_train_test_val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Karishni\\Desktop\\Spring-Semester\\DDP\\.venv\\Lib\\site-packages\\datasets\\load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Karishni\\Desktop\\Spring-Semester\\DDP\\.venv\\Lib\\site-packages\\datasets\\load.py:756: FutureWarning: The repository for precision contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/precision/precision.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Karishni\\Desktop\\Spring-Semester\\DDP\\.venv\\Lib\\site-packages\\datasets\\load.py:756: FutureWarning: The repository for recall contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/recall/recall.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Karishni\\Desktop\\Spring-Semester\\DDP\\.venv\\Lib\\site-packages\\datasets\\load.py:756: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/f1/f1.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "accuracy_metric = load_metric('accuracy')\n",
    "precision_metric = load_metric('precision')\n",
    "recall_metric = load_metric('recall')\n",
    "f1_metric = load_metric('f1')\n",
    "\n",
    "#function to compute the metrics of the model\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(-1)\n",
    "    acc = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    prec = precision_metric.compute(predictions=predictions, references=labels, average='macro')\n",
    "    rec = recall_metric.compute(predictions=predictions, references=labels, average='macro')\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average='macro')\n",
    "    return {\n",
    "        'accuracy': acc['accuracy'],\n",
    "        'precision': prec['precision'],\n",
    "        'recall': rec['recall'],\n",
    "        'f1': f1['f1']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Karishni\\Desktop\\Spring-Semester\\DDP\\.venv\\Lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import BigBirdForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Load the model\n",
    "model = BigBirdForSequenceClassification.from_pretrained('google/bigbird-roberta-base', num_labels=4)  # Adjust num_labels accordingly\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=final_datasets['train'],\n",
    "    eval_dataset=final_datasets['validation'],\n",
    "    compute_metrics=compute_metrics  # Assuming compute_metrics function is defined as shown earlier\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate(final_datasets['test'])\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
